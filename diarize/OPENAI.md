Novel Speaker Diarization Boundary Refinement Techniques (2024–2026)

1. Boundary Refinement as a Separate Stage in Diarization

Traditional diarization pipelines often included a post-processing resegmentation step to fine-tune speaker boundaries after initial clustering. For example, a Variational Bayes HMM (VB-HMM) could use initial cluster assignments as states to refine segment boundaries, yielding significant error reduction ￼ ￼. Modern research continues to explore dedicated boundary-refinement stages: one notable approach is to decouple “who spoke” from “when” and refine the “when” (boundaries) with a second-pass model. For instance, Target-Speaker Voice Activity Detection (TS-VAD) has been used as a post-processing module: starting from an initial diarization output, TS-VAD refines the time boundaries for each speaker by focusing on one speaker at a time (given that speaker’s embedding) ￼ ￼. This effectively treats boundary localization as its own stage, improving recall of speaker turns. More recently, generative modeling techniques have been applied to boundary refinement. Chen et al. (2024) introduced a Flow-Matching-based refinement called Flow-TSVAD, which uses a generative flow model on the diarization label sequence to iteratively improve segment timing ￼ ￼. In their setup, an initial diarization (from a Seq2Seq TS-VAD system) produces a coarse binary speaker timeline; Flow-TSVAD then maps this binary sequence into a continuous latent space and iteratively adjusts the boundaries via a learned generative process, yielding better alignment with true speaker changes ￼ ￼. This two-stage approach (discriminative first pass, generative refinement second pass) achieved lower error than the baseline, demonstrating that treating boundary localization as a separate, refineable task can be beneficial. In summary, while many end-to-end systems implicitly handle boundaries, there is a growing interest in explicit boundary-refinement stages – whether via classic HMM resegmentation or novel neural methods (like TS-VAD or diffusion-style models) that revisit and adjust “who speaks when” after an initial pass.

2. Conditioning on Speaker Identity for Boundary Detection

Yes – several works have leveraged known or estimated speaker embeddings to guide boundary placement. The TS-VAD approach mentioned above is a prime example: it takes speaker embeddings (enrollments) as input and produces voice activity timelines for each target speaker, thus using speaker identity to pinpoint when that speaker starts/stops talking ￼ ￼. In practice, diarization systems will often perform an initial clustering to obtain rough speaker embeddings, then feed those into a TS-VAD network to re-detect speech segments for each speaker, greatly refining the diarization. This separation of concerns (first who via clustering, then when for each who via TS-VAD) has proven very effective – TS-VAD-based systems have repeatedly won diarization competitions due to their improved accuracy on speaker transition boundaries ￼ ￼. Extensions of this idea in 2024–2025 include a Transformer-based TS-VAD that can handle a variable number of speakers by encoding time and speaker axes dynamically ￼, and even end-to-end TS-VAD models that jointly learn the speaker embeddings and the diarization refinement in one network ￼. These methods still conceptually condition on speaker identity – e.g. by maintaining a memory of each speaker’s voice – to decide if/when a speaker change occurs. Aside from TS-VAD, another example is the use of speaker-conditioned segmentation models. Classic clustering+HMM resegmentation can be viewed this way: the HMM uses each cluster’s speaker model (often an embedding-based GMM) to reassign frames, effectively sliding segment boundaries to better fit the known speaker characteristics ￼ ￼. In contemporary research, the notion of speaker-conditioned boundary detection is sometimes implicit – for instance, EEND-EDA (Encoder-Decoder Attractor EEND) creates attractors representing different speakers and uses them to output frame-level speaker activations, which is a form of conditioning on discovered speaker identities. Overall, conditioning on speaker embeddings to refine “when” a speaker’s speech starts or ends is a well-established idea: it appears in clustering+resegmentation pipelines, in TS-VAD refiners, and in newer neural architectures that separate the speaker identification from temporal localization.

3. Diffusion Models for Audio Segmentation (Speaker Change Detection)

The application of denoising diffusion or other generative models to diarization is very fresh in the literature. In late 2024, researchers began to explore treating diarization as a sequence generation problem rather than a pure classification problem ￼ ￼. For example, Flow-TSVAD (Chen et al., 2024) leverages a Flow Matching algorithm – conceptually related to diffusion probabilistic models – to generate/refine the sequence of speaker labels over time ￼. Notably, the authors found that applying a diffusion-like process directly on the binary speaker label sequence was ineffective, so they first trained an autoencoder to map the discrete labels into a continuous latent representation of the diarization timeline ￼ ￼. They then performed flow-matching in that latent space to iteratively nudge the segmentation toward a more likely (and accurate) configuration. The result was a diarization system that outperformed the baseline Seq2Seq model, and it could even sample multiple hypothesized diarizations and ensemble them for further gains ￼. This is likely the first use of a diffusion-like generative approach for temporal boundary prediction in speech. It was inspired by analogous successes in other domains – the authors cite diffusion models being used for object detection and instance segmentation in vision, which similarly treat bounding boxes or masks as something to be generated ￼ ￼. As of 2025, fully denoising-diffusion diarization models are still rare, but the Flow-TSVAD work suggests a promising direction: generative modeling can provide a flexible way to refine boundaries, sample alternative segmentations, and potentially capture uncertainty in “who spoke when” ￼. We might soon see diffusion or other score-based models applied directly to speaker change detection – for example, one could imagine a diffusion model that “denoises” an initially rough speaker change probability curve into a sharper boundary indicator. In summary, while not yet common, there is early evidence that diffusion-style generative techniques can be applied to audio segmentation, with Flow-TSVAD demonstrating improved diarization via a flow-matching (continuous diffusion) approach ￼ ￼.

4. State-of-the-Art Diarization Benchmarks (2024–2026)

Diarization Error Rate (DER) on standard datasets has seen steady improvements from 2024 to 2025. Here are some of the latest benchmark figures and SOTA systems:
	•	AMI Meeting Corpus (English meetings) – Top models achieve roughly 15–20% DER on AMI (which is evaluated with strict overlap handling). For example, the Pyannote 3.1 toolkit (with a neural segmentation+clustering pipeline) reports about 19% DER on AMI in recent publications ￼. End-to-end models are closing the gap; Härkönen et al. (2024) introduced EEND-M2F, an Mask2Former-based diarization model, which set a new state of the art on AMI (and other meeting corpora) ￼ – exact AMI DER wasn’t quoted in their abstract, but qualitatively it’s on par or better than 19%. Notably, EEND-M2F is fully end-to-end (no clustering needed) and it drew inspiration from image segmentation (Mask2Former) to iteratively refine speaker masks, yielding SOTA performance ￼.
	•	VoxConverse (Audio from YouTube videos) – This dataset features challenging multi-speaker media audio. The best reported DERs here are in the low teens. A recent benchmarking study found Pyannote-based systems around 11% DER on VoxConverse ￼, which is considered excellent. (VoxConverse has many speakers per recording, but often clear audio.) Indeed, Pyannote’s commercial API version (“PyannoteAI”) averaged 11.2% DER overall across a mix of evaluation data ￼, largely thanks to strong performance on VoxConverse and similar sets. Hybrid models like DiariZen – which combines local EEND with WavLM embeddings and clustering – also score around 12–13% DER on VoxConverse ￼. In summary, as of 2025, ~10–12% DER is the state-of-art on this benchmark.
	•	DIHARD III (very hard, conversational speech) – DIHARD is notoriously difficult (wide domain diversity, heavy overlap, noisy audio). The 2021 DIHARD III challenge winning system had about 17% DER. Until recently, that was hard to beat. In 2024, the EEND-M2F model achieved 16.07% DER on DIHARD-III evaluation – “the first major improvement upon the challenge winning system” according to the authors ￼. This is essentially the new SOTA (and it doesn’t rely on clustering or cascading modules). Most other top systems for DIHARD (2023–2024) report DER in the 20–25% range. For instance, Pyannote 3.1 achieved around 27% DER on DIHARD III in one evaluation ￼, and other strong research systems fall in the low 20s. (The DIHARD baseline, for context, was 35–45% DER ￼ – showing how much progress has been made.) In addition to DER, some works report the Jaccard Error Rate (JER) for boundary accuracy on DIHARD – these too have improved alongside DER. A recent analysis noted that a significant portion of the remaining diarization errors come from missed speech (missed boundaries) rather than speaker mis-assignments ￼, reflecting that boundaries are still the “long pole” on DIHARD.

To summarize: on AMI (~4 speakers, clean meetings) we now see DER ≈ 15–20%; on VoxConverse (open-domain) best DER ≈ 11%; and on DIHARD III (open-domain, messy) best DER ≈ 16%. The SOTA systems span end-to-end neural models (like EEND-M2F) and advanced hybrid pipelines (Pyannote, TS-VAD-based, or mixture-of-experts). Many of these leverage large pre-trained models (e.g. WavLM features, see Section 7) to reach these low error rates.

5. Commercial Diarization Systems (ElevenLabs Scribe, AssemblyAI, Deepgram)

Commercial transcription APIs such as ElevenLabs Scribe, AssemblyAI, Deepgram, Speechmatics, etc., have invested in diarization as a key feature, often surpassing open-source tools in difficult conditions. While details are often proprietary, we do have some information:
	•	ElevenLabs Scribe (2025) – Scribe is primarily an ASR model, but it includes “smart speaker diarization” integrated into the transcription process ￼. It supports up to 32 distinct speakers labeled in a single audio stream ￼, indicating a highly flexible diarization component. Scribe provides word-level timestamps with speaker labels, and even tags non-speech events like “[laughter]” in-line ￼ ￼. ElevenLabs hasn’t published a paper on Scribe’s architecture, but it presumably uses a large transformer-based ASR (trained on massive multilingual data) with an internal diarization mechanism. This could be realized by multi-task learning (predicting who-spoke-when alongside the transcription) or by a pipeline where an internal diarizer assigns speaker IDs to the recognized words. The impressive 32-speaker capacity and word-level alignment suggest that Scribe’s diarization is fully integrated with decoding – possibly using speaker embeddings and attention to continuously identify speakers as it transcribes. In practice, Scribe has been reported to achieve extremely high overall accuracy (it claims state-of-the-art ASR quality in many languages), so its diarization is likely on par with or better than the best research systems in real-world scenarios.
	•	AssemblyAI and Deepgram – These companies also offer speech-to-text APIs with diarization. AssemblyAI in particular announced improvements in 2025 via a new speaker embedding model that gave a 30% diarization accuracy boost in noisy conditions ￼. This suggests they focused on training a robust speaker encoder (possibly using self-supervised learning or large proprietary datasets) to handle challenging audio. Commercial systems often combine techniques: for example, they may use a powerful embedding extractor (derived from huge speaker-ID datasets), followed by clustering or a neural assignment model, all optimized on large-scale internal data. Deepgram has emphasized speed and real-time operation; their diarization likely uses a low-latency streaming clustering algorithm paired with an ASR. Speechmatics and Rev.ai reportedly leverage LM contextualization – e.g. using the transcript content to help decide speaker turns (like inserting breaks when the dialog context changes speaker).

What sets these commercial systems apart is engineering and scale: they train on vast amounts of proprietary data (e.g. varied meetings, phone calls, videos) and tune for real-world conditions. They often incorporate multi-modal cues or metadata – for example, if punctuation or ASR confidence suggests a turn change, they might adjust speaker labels accordingly (some APIs allow diarization threshold settings to control how readily the model splits speakers ￼). ElevenLabs explicitly mentions “context-aware” diarization, implying the model uses conversational context to avoid mistakenly splitting one speaker into two ￼. In sum, commercial diarizers in 2024–2025 achieve top-tier performance through a mix of large-scale training, integrated ASR+diarization modeling, and practical tricks (like dynamic thresholds, NLP-informed boundary adjustment, and aggressive model ensembling). While exact DER figures aren’t always published, user reports indicate these services can diarize difficult audio with accuracy unmatched by out-of-the-box open models. (For instance, Scribe’s diarization quality has been praised anecdotally; and the BrassTranscripts service – likely using a custom pipeline – advertises “highest accuracy”, presumably combining ASR and diarization optimally ￼ ￼.) As these systems are closed-source, the community often learns from them indirectly – e.g. by incorporating similar large pretrained models or by using their outputs to benchmark open solutions.

6. Hierarchical & Multi-Stage Diarization (Coarse-to-Fine)

There is a clear trend toward multi-stage diarization frameworks that break the problem into coarse and fine sub-tasks. One example is the USTC system for the CHiME-8 Challenge (2024), which explicitly has three diarization stages ￼ ￼:
	•	Stage 1: Speaker change detection & speech separation. They first run an overlap detector and then apply Continuous Speech Separation (CSS) on multi-microphone audio to isolate single-speaker segments ￼. These cleaner segments are fed into a clustering-based diarization (CSD) module to produce an initial “who spoke when” (with fewer overlaps). This yields a coarse diarization hypothesis with high recall of speech segments but possibly some speaker errors. Then they do an initial neural diarization (NSD) decoding on this output ￼.
	•	Stage 2: Mask-based refinement. Using the initial diarization, they derive binary speaker masks (like time-frequency masks per speaker) and initialize a complex Angular Central GMM (cACGMM) to re-estimate speaker assignments on the original multi-channel audio ￼. This produces refined time-frequency masks for each speaker. They convert these masks into per-speaker VAD (speech activity) via “Mask-to-VAD” processing, which reduces speaker mis-assignments (lowering speaker error) ￼. With this improved segmentation, they run a second NSD decoding. Essentially, Stage 2 uses the initial guess to guide a finer segmentation, leveraging acoustic separation to correct some boundary placements and splits/merges based on speaker characteristics.
	•	Stage 3: Guided source separation & re-clustering. The output of Stage 2 (improved diarization) is then used to perform Guided Source Separation (GSS) – producing even cleaner per-speaker audio streams ￼. Very short utterances are filtered out, and the remaining segments are re-clustered and passed through a final diarization decoding to produce the final result ￼. This final stage acts as a fine resolution cleanup, consolidating or splitting segments as needed and ensuring each speaker’s audio is grouped correctly.

This three-stage coarse-to-fine strategy resulted in a highly effective system (it improved DER progressively at each stage, as reported) ￼. It exemplifies hierarchical diarization: early stages focus on recall (finding all possible boundaries), perhaps over-segmenting, and later stages focus on precision (adjusting or merging boundaries using more informed models).

Another example is EEND-VC (End-to-End Neural Diarization with Vector Clustering), which is a two-stage hybrid: Stage 1 runs a local EEND model on short chunks to get fine-grained speaker activity; Stage 2 performs global clustering on the embeddings from those chunks to associate the same speaker across different windows ￼. This can be seen as coarse speaker grouping after fine local segmentation – effectively a hierarchical approach that first diarizes short regions (high detail, limited context) then ties together those results over a long recording (coarser grouping). The EEND-VC method showed that combining a strong local neural diarizer with a global clustering step can handle long recordings with many speakers better than either method alone ￼.

We also see hierarchical ideas in Google’s diarization research: for instance, a 2023 Google system used a streaming EEND model with an “arrival-order speaker cache”, which is conceptually a hierarchy between a short-term diarization and a longer-term speaker-tracking memory ￼. And classic HMM-based diarization can be viewed as hierarchical in time: first a coarse segmentation (fixed-length or change-detection-based) then a fine resegmentation with an HMM using frame-level features ￼ ￼.

In summary, coarse-to-fine diarization is a common theme in SOTA systems: they might start with an over-segmentation or separation to get candidate speaker turns (coarse boundaries), then apply more informed models (using speaker embeddings, ASR transcripts, or separated audio) to refine those boundaries and cluster speakers at a finer granularity. This bidirectional refinement – e.g. splitting or merging segments by looking at both preceding and following context (bidirectional analysis) – is facilitated in multi-stage setups. Later stages can correct earlier decisions by leveraging global knowledge (like “Speaker X hasn’t appeared for 10 minutes, maybe merge those segments with Speaker Y” or “These two adjacent segments sound like the same speaker, so remove the boundary”). We expect future diarization frameworks to continue this trend, possibly with hierarchical neural networks that internally implement coarse-to-fine processing (some recent transformer architectures attempt exactly that by multi-scale attention).

7. Improving Sample Efficiency in Diarization

Labeling diarization data (“who spoke when” in long audios) is expensive, so researchers have aimed to reduce the need for large labeled training sets through smarter architectures and use of unlabelled data:
	•	Self-Supervised Pretraining: The biggest gains in recent years come from using self-supervised learning (SSL) audio models (like WavLM, Hubert, etc) as front-ends for diarization. These models are trained on enormous amounts of unlabeled audio and learn rich speech representations. Incorporating them has dramatically improved diarization accuracy and robustness ￼ – effectively doing more with less task-specific data. A 2025 study by Han et al. notes that WavLM has become a “cornerstone” of state-of-the-art diarization systems, substantially reducing DER on complex audio ￼. Crucially, SSL features make diarization models more sample-efficient: teams report that fine-tuning a pretrained WavLM on a relatively small diarization dataset can outperform a model trained from scratch on a much larger set. In fact, Han et al. (2025) showed that you can prune a WavLM-based diarization model to 20% of its size and still match or surpass uncompressed performance, and that using only 50% of the available training data was enough to maintain accuracy ￼. In other words, a well-chosen architecture (here, a compressed WavLM with distillation) required only half the usual data and still did not degrade – a clear win for sample efficiency.
	•	Unsupervised and Semi-Supervised Learning: Beyond pretraining, researchers use unlabeled or weakly labeled data to improve diarization. One approach is domain adaptation: e.g., NTT’s 2023 system used unsupervised adaptation with a quality-aware masking to fine-tune a TS-VAD model on untranscribed audio from the target domain (meetings), which improved performance without additional labels (this was noted in the DIHARD/CHiME challenges). Other semi-supervised methods generate pseudo-labels from easier tasks to train the diarizer – for instance, a 2024 study leveraged production movie scripts to get approximate speaker timestamps in audio, greatly expanding training data with “free” labels ￼. Graph-based semi-supervised learning has also been explored (using a small labeled set and a larger unlabeled set together with graph neural nets to propagate speaker identity info). These techniques aim to squeeze more out of limited ground truth labels.
	•	Data Augmentation & Simulation: While the question asks beyond just augmentation, it’s worth noting a common practice: create synthetic multi-speaker recordings by mixing single-speaker recordings. This was pioneered in EEND papers and is still used – it enables virtually unlimited training examples without manual labeling. For example, simulate conversations by randomly mixing Librispeech utterances for 2, 3, 4 speakers; the model learns diarization from these artificial mixtures. Although synthetic, this greatly improves baseline models and reduces the amount of real annotated data needed.
	•	Architectural Improvements: Some architectures inherently need fewer examples to generalize. For instance, memory-enhanced models (like the NSD-MA-MSE in 2023 ￼) can leverage speaker memory across long contexts, making effective use of each training example’s information. Large margin losses or speaker-aware objectives can also improve sample efficiency by making better use of each training sample (ensuring the model really separates speakers in embedding space). Moreover, multi-task learning (jointly training diarization with ASR or with speaker recognition objectives) can allow cross-training on more data, indirectly benefiting diarization with minimal direct labels.

In summary, the state of the art has significantly improved sample efficiency for speaker diarization. By using powerful SSL front-ends (which carry the burden of learning speech patterns from huge unlabeled corpora), today’s diarization models can achieve strong results with far less task-specific data than a few years ago ￼ ￼. Unsupervised and semi-supervised techniques further cut down labeled data needs by exploiting unlabeled audio in clever ways. The result is that researchers can approach or even surpass prior benchmarks using only a fraction of the previously required training data. This is encouraging for low-resource languages or novel domains – one can leverage a pretrained model like WavLM and adapt it to diarization with limited annotation, yet still get high accuracy. Going forward, we may see end-to-end diarization adapted with few-shot learning (e.g. adapting to a new environment with a handful of recorded meetings) and continued use of large foundation models (like SpeakerLM, which combines ASR and diarization in one LLM-style model ￼) to minimize the need for costly annotations. Each architectural innovation that better models speaker characteristics or leverages context ultimately means fewer examples are needed to teach the model “who spoke when.”

Sources: Recent papers and results have been referenced inline, including Flow-TSVAD for generative diarization ￼ ￼, the Mask2Former-based EEND-M2F model ￼, multi-stage diarization from CHiME-8 ￼ ￼, analysis of diarization errors and benchmark performances ￼ ￼, and documentation of ElevenLabs Scribe’s features ￼ ￼, among others. Each citation points to the source of the specific detail for further reading.