"""
JSON Dataset for the JSON Repair Engine.

Generates (clean_json, corrupted_json, sigma) triples for training.

Corruption types:
=== Token-level (after tokenization) ===
1. Missing comma/colon
2. Extra comma/colon
3. Missing brace/bracket
4. Extra brace/bracket
5. Swapped tokens
6. Masked tokens

=== Text-level (before tokenization) ===
7. Python constants (True/False/None)
8. Single quotes instead of double
9. Unquoted keys
10. Markdown fences
11. Prose wrapping
12. Comments (line/block)
13. Truncation
14. Multi-error combinations
"""

import json
import random
import re
import torch
from torch.utils.data import Dataset
from typing import List, Tuple, Optional, Dict, Callable
from tokenizer_json import JSONTokenizer


# =============================================================================
# TEXT-LEVEL CORRUPTIONS (applied before tokenization)
# =============================================================================

def corrupt_python_true(s: str) -> str:
    """Replace 'true' with Python 'True'."""
    return s.replace('true', 'True')

def corrupt_python_false(s: str) -> str:
    """Replace 'false' with Python 'False'."""
    return s.replace('false', 'False')

def corrupt_python_null(s: str) -> str:
    """Replace 'null' with Python 'None'."""
    return s.replace('null', 'None')

def corrupt_single_quotes(s: str) -> str:
    """Replace double quotes with single quotes."""
    return s.replace('"', "'")

def corrupt_unquote_keys(s: str) -> str:
    """Remove quotes from simple keys: {"name": -> {name:"""
    return re.sub(r'"(\w+)":', r'\1:', s)

def corrupt_markdown_fence(s: str) -> str:
    """Wrap JSON in markdown code fence."""
    return f'```json\n{s}\n```'

def corrupt_markdown_fence_plain(s: str) -> str:
    """Wrap JSON in plain markdown code fence."""
    return f'```\n{s}\n```'

def corrupt_prose_wrap(s: str) -> str:
    """Add LLM-style prose around JSON."""
    intros = [
        "Here is the JSON:",
        "The data is:",
        "Here's what I found:",
        "Result:",
    ]
    outros = [
        "Hope this helps!",
        "Let me know if you need anything else.",
        "",
        "That's all the data.",
    ]
    return f"{random.choice(intros)}\n{s}\n{random.choice(outros)}"

def corrupt_line_comment(s: str) -> str:
    """Add a line comment at the end."""
    comments = [
        "// generated",
        "// user data",
        "// TODO: validate",
        "// config",
    ]
    return s + ' ' + random.choice(comments)

def corrupt_block_comment(s: str) -> str:
    """Add a block comment."""
    comments = [
        "/* data */",
        "/* generated by AI */",
        "/* config block */",
    ]
    return s + ' ' + random.choice(comments)

def corrupt_truncate(s: str) -> str:
    """Truncate the JSON string randomly."""
    if len(s) < 5:
        return s
    cut = random.randint(1, min(15, len(s) // 3))
    return s[:-cut]

def corrupt_trailing_comma_obj(s: str) -> str:
    """Add trailing comma in object: {"a": 1,}"""
    # Find } preceded by non-comma and add comma
    return re.sub(r'([^,\s])\s*}', r'\1,}', s)

def corrupt_trailing_comma_arr(s: str) -> str:
    """Add trailing comma in array: [1, 2,]"""
    return re.sub(r'([^,\s])\s*]', r'\1,]', s)


# Registry of text-level corruptions with their categories
TEXT_CORRUPTIONS: List[Tuple[str, str, Callable[[str], str]]] = [
    # (name, category, function)
    ('python_true', 'values', corrupt_python_true),
    ('python_false', 'values', corrupt_python_false),
    ('python_null', 'values', corrupt_python_null),
    ('single_quotes', 'quotes', corrupt_single_quotes),
    ('unquote_keys', 'quotes', corrupt_unquote_keys),
    ('markdown_fence', 'llm_specific', corrupt_markdown_fence),
    ('markdown_fence_plain', 'llm_specific', corrupt_markdown_fence_plain),
    ('prose_wrap', 'llm_specific', corrupt_prose_wrap),
    ('line_comment', 'llm_specific', corrupt_line_comment),
    ('block_comment', 'llm_specific', corrupt_block_comment),
    ('truncate', 'structural', corrupt_truncate),
    ('trailing_comma_obj', 'punctuation', corrupt_trailing_comma_obj),
    ('trailing_comma_arr', 'punctuation', corrupt_trailing_comma_arr),
]

# Common multi-error combinations (simulating real-world patterns)
MULTI_ERROR_COMBOS = [
    # LLM output with Python bools
    ['markdown_fence', 'python_true', 'python_false'],
    # JS-style JSON
    ['single_quotes', 'unquote_keys'],
    # Python dict printed as JSON
    ['python_true', 'python_false', 'python_null'],
    # Truncated LLM response
    ['markdown_fence', 'truncate'],
    # Prose with Python constants
    ['prose_wrap', 'python_null'],
]


# Sample JSON templates for generating training data
JSON_TEMPLATES = [
    # Simple key-value
    '{"key": "value"}',
    '{"name": "Alice", "age": 30}',
    '{"active": true, "count": 0}',

    # Nested objects
    '{"user": {"id": 1, "name": "Bob"}}',
    '{"config": {"debug": false, "level": 5}}',

    # Arrays
    '[1, 2, 3]',
    '["a", "b", "c"]',
    '[true, false, null]',

    # Mixed
    '{"items": [1, 2, 3], "total": 6}',
    '{"users": [{"id": 1}, {"id": 2}]}',

    # Larger structures
    '{"a": 1, "b": 2, "c": 3, "d": 4}',
    '{"nested": {"deep": {"value": 42}}}',
]


def generate_random_json(
    max_depth: int = 3,
    max_keys: int = 5,
    max_array_len: int = 5,
    max_string_len: int = 10,
) -> str:
    """
    Generate a random valid JSON string.

    Args:
        max_depth: Maximum nesting depth
        max_keys: Maximum keys per object
        max_array_len: Maximum array length
        max_string_len: Maximum string length

    Returns:
        Valid JSON string
    """
    def random_string():
        length = random.randint(1, max_string_len)
        chars = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_'
        return ''.join(random.choice(chars) for _ in range(length))

    def random_number():
        if random.random() < 0.5:
            return random.randint(-1000, 1000)
        else:
            return round(random.uniform(-100, 100), 2)

    def random_value(depth: int):
        if depth >= max_depth:
            # Only primitives at max depth
            choice = random.randint(0, 4)
        else:
            choice = random.randint(0, 6)

        if choice == 0:
            return random_string()
        elif choice == 1:
            return random_number()
        elif choice == 2:
            return True
        elif choice == 3:
            return False
        elif choice == 4:
            return None
        elif choice == 5:
            # Object
            num_keys = random.randint(1, max_keys)
            return {random_string(): random_value(depth + 1) for _ in range(num_keys)}
        else:
            # Array
            length = random.randint(0, max_array_len)
            return [random_value(depth + 1) for _ in range(length)]

    # Start with either object or array
    if random.random() < 0.7:
        num_keys = random.randint(1, max_keys)
        data = {random_string(): random_value(1) for _ in range(num_keys)}
    else:
        length = random.randint(1, max_array_len)
        data = [random_value(1) for _ in range(length)]

    return json.dumps(data, separators=(',', ':'))


class TextCorruptionEngine:
    """
    Engine for text-level JSON corruption (before tokenization).

    Handles corruptions that affect the raw string, such as:
    - Python constants (True/False/None)
    - Single quotes
    - Unquoted keys
    - Markdown fences
    - Comments
    - Truncation
    """

    def __init__(self):
        self.corruptions = {name: fn for name, cat, fn in TEXT_CORRUPTIONS}
        self.corruption_names = [name for name, cat, fn in TEXT_CORRUPTIONS]
        self.categories = {}
        for name, cat, fn in TEXT_CORRUPTIONS:
            if cat not in self.categories:
                self.categories[cat] = []
            self.categories[cat].append(name)

    def corrupt(
        self,
        text: str,
        num_corruptions: int = 1,
        corruption_types: Optional[List[str]] = None,
        use_combo: bool = False,
    ) -> Tuple[str, List[str]]:
        """
        Apply text-level corruptions to JSON string.

        Args:
            text: Clean JSON string
            num_corruptions: Number of corruptions to apply
            corruption_types: Specific corruptions to use, or None for random
            use_combo: If True, use a predefined multi-error combo

        Returns:
            (corrupted_text, list of applied corruption names)
        """
        applied = []

        if use_combo and random.random() < 0.3:
            # Use a predefined multi-error combination
            combo = random.choice(MULTI_ERROR_COMBOS)
            for ctype in combo:
                if ctype in self.corruptions:
                    text = self.corruptions[ctype](text)
                    applied.append(ctype)
            return text, applied

        # Random or specified corruptions
        if corruption_types is None:
            corruption_types = random.choices(self.corruption_names, k=num_corruptions)

        for ctype in corruption_types:
            if ctype in self.corruptions:
                text = self.corruptions[ctype](text)
                applied.append(ctype)

        return text, applied

    def corrupt_by_category(
        self,
        text: str,
        category: str,
        num_corruptions: int = 1,
    ) -> Tuple[str, List[str]]:
        """Apply corruptions from a specific category."""
        if category not in self.categories:
            return text, []

        corruption_types = random.choices(self.categories[category], k=num_corruptions)
        return self.corrupt(text, corruption_types=corruption_types)


class JSONCorruptionEngine:
    """
    Engine for corrupting JSON in realistic ways.

    Corruption operations:
    - delete_comma: Remove a comma
    - insert_comma: Add an extra comma
    - delete_colon: Remove a colon
    - insert_colon: Add an extra colon
    - delete_brace: Remove { or }
    - insert_brace: Add extra { or }
    - delete_bracket: Remove [ or ]
    - insert_bracket: Add extra [ or ]
    - delete_quote: Remove a quote from a string
    - truncate_string: Cut off string content
    - swap_adjacent: Swap two adjacent tokens
    - mask_token: Replace with <MASK>
    """

    def __init__(self, tokenizer: JSONTokenizer):
        self.tokenizer = tokenizer

    def corrupt(
        self,
        token_ids: List[int],
        sigma: float,
        corruption_type: Optional[str] = None,
    ) -> Tuple[List[int], str]:
        """
        Corrupt a token sequence.

        Args:
            token_ids: Clean token sequence
            sigma: Corruption intensity (0-1)
            corruption_type: Specific corruption to apply, or None for random

        Returns:
            (corrupted_ids, corruption_name)
        """
        # Number of corruptions based on sigma
        # sigma=0.1 -> ~1 corruption, sigma=0.5 -> ~3 corruptions
        num_corruptions = max(1, int(sigma * 5))

        corrupted = list(token_ids)

        corruption_types = [
            'delete_comma',
            'insert_comma',
            'delete_colon',
            'insert_colon',
            'delete_brace',
            'delete_bracket',
            'swap_adjacent',
            'mask_token',
        ]

        applied = []

        for _ in range(num_corruptions):
            if corruption_type:
                ctype = corruption_type
            else:
                ctype = random.choice(corruption_types)

            if ctype == 'delete_comma' and self._has_token(corrupted, self.tokenizer.comma_id):
                corrupted = self._delete_token(corrupted, self.tokenizer.comma_id)
                applied.append('delete_comma')

            elif ctype == 'insert_comma':
                corrupted = self._insert_token(corrupted, self.tokenizer.comma_id)
                applied.append('insert_comma')

            elif ctype == 'delete_colon' and self._has_token(corrupted, self.tokenizer.colon_id):
                corrupted = self._delete_token(corrupted, self.tokenizer.colon_id)
                applied.append('delete_colon')

            elif ctype == 'insert_colon':
                corrupted = self._insert_token(corrupted, self.tokenizer.colon_id)
                applied.append('insert_colon')

            elif ctype == 'delete_brace':
                if self._has_token(corrupted, self.tokenizer.lbrace_id):
                    if random.random() < 0.5:
                        corrupted = self._delete_token(corrupted, self.tokenizer.lbrace_id)
                    else:
                        corrupted = self._delete_token(corrupted, self.tokenizer.rbrace_id)
                    applied.append('delete_brace')

            elif ctype == 'delete_bracket':
                if self._has_token(corrupted, self.tokenizer.lbracket_id):
                    if random.random() < 0.5:
                        corrupted = self._delete_token(corrupted, self.tokenizer.lbracket_id)
                    else:
                        corrupted = self._delete_token(corrupted, self.tokenizer.rbracket_id)
                    applied.append('delete_bracket')

            elif ctype == 'swap_adjacent':
                corrupted = self._swap_adjacent(corrupted)
                applied.append('swap_adjacent')

            elif ctype == 'mask_token':
                corrupted = self._mask_random(corrupted)
                applied.append('mask_token')

        return corrupted, '+'.join(applied) if applied else 'none'

    def _has_token(self, ids: List[int], token_id: int) -> bool:
        """Check if token exists in sequence (excluding special tokens)."""
        # Skip BOS and EOS positions
        return token_id in ids[1:-1]

    def _delete_token(self, ids: List[int], token_id: int) -> List[int]:
        """Delete one occurrence of a token."""
        # Find all positions (excluding BOS/EOS)
        positions = [i for i, t in enumerate(ids) if t == token_id and 0 < i < len(ids) - 1]
        if not positions:
            return ids
        pos = random.choice(positions)
        return ids[:pos] + ids[pos + 1:]

    def _insert_token(self, ids: List[int], token_id: int) -> List[int]:
        """Insert a token at a random position."""
        # Insert between BOS and EOS
        if len(ids) <= 2:
            return ids
        pos = random.randint(1, len(ids) - 1)
        return ids[:pos] + [token_id] + ids[pos:]

    def _swap_adjacent(self, ids: List[int]) -> List[int]:
        """Swap two adjacent tokens."""
        if len(ids) <= 3:
            return ids
        # Pick a position (excluding BOS and last position before EOS)
        pos = random.randint(1, len(ids) - 3)
        result = list(ids)
        result[pos], result[pos + 1] = result[pos + 1], result[pos]
        return result

    def _mask_random(self, ids: List[int]) -> List[int]:
        """Replace a random token with MASK."""
        if len(ids) <= 2:
            return ids
        pos = random.randint(1, len(ids) - 2)
        result = list(ids)
        result[pos] = self.tokenizer.mask_id
        return result


class JSONRepairDataset(Dataset):
    """
    Dataset for training the JSON denoiser.

    Generates (clean, corrupted, sigma) triples with both text-level
    and token-level corruptions.
    """

    def __init__(
        self,
        num_samples: int,
        max_len: int = 128,
        sigma_min: float = 0.1,
        sigma_max: float = 0.8,  # Increased from 0.5 for more multi-error
        use_templates: bool = True,
        max_depth: int = 3,
        seed: Optional[int] = None,
        text_corruption_prob: float = 0.5,  # Probability of text-level corruption
        multi_error_prob: float = 0.3,  # Probability of multi-error combo
    ):
        """
        Args:
            num_samples: Number of samples to generate
            max_len: Maximum sequence length (will pad/truncate)
            sigma_min: Minimum corruption intensity
            sigma_max: Maximum corruption intensity (default 0.8 for up to 6 corruptions)
            use_templates: If True, mix templates with random JSON
            max_depth: Max nesting depth for random JSON
            seed: Random seed for reproducibility
            text_corruption_prob: Probability of applying text-level corruptions
            multi_error_prob: Probability of using predefined multi-error combos
        """
        if seed is not None:
            random.seed(seed)

        self.num_samples = num_samples
        self.max_len = max_len
        self.sigma_min = sigma_min
        self.sigma_max = sigma_max
        self.text_corruption_prob = text_corruption_prob
        self.multi_error_prob = multi_error_prob

        self.tokenizer = JSONTokenizer()
        self.token_corruption_engine = JSONCorruptionEngine(self.tokenizer)
        self.text_corruption_engine = TextCorruptionEngine()

        # Generate clean JSON samples
        self.clean_jsons = []
        for i in range(num_samples):
            if use_templates and random.random() < 0.3:
                # Use a template
                json_str = random.choice(JSON_TEMPLATES)
            else:
                # Generate random JSON
                json_str = generate_random_json(max_depth=max_depth)
            self.clean_jsons.append(json_str)

        if seed is not None:
            random.seed()  # Reset seed

    def __len__(self) -> int:
        return self.num_samples

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Returns:
            clean: (max_len,) clean token sequence
            corrupted: (max_len,) corrupted token sequence
            sigma: scalar corruption intensity

        Corruption pipeline:
        1. Start with clean JSON string
        2. Optionally apply text-level corruptions (Python constants, quotes, fences, etc.)
        3. Tokenize the (possibly corrupted) string
        4. Apply token-level corruptions (missing commas, braces, etc.)
        """
        json_str = self.clean_jsons[idx]

        # Tokenize clean JSON (this is the target)
        clean_ids = self.tokenizer.tokenize(json_str)

        # Sample corruption intensity
        sigma = random.uniform(self.sigma_min, self.sigma_max)
        num_text_corruptions = max(1, int(sigma * 3))  # 1-3 text corruptions
        num_token_corruptions = max(1, int(sigma * 5))  # 1-6 token corruptions

        # Start with clean string for corruption
        corrupted_str = json_str

        # Step 1: Apply text-level corruptions (with probability)
        if random.random() < self.text_corruption_prob:
            use_combo = random.random() < self.multi_error_prob
            corrupted_str, _ = self.text_corruption_engine.corrupt(
                corrupted_str,
                num_corruptions=num_text_corruptions,
                use_combo=use_combo,
            )

        # Step 2: Tokenize the corrupted string
        corrupted_ids = self.tokenizer.tokenize(corrupted_str)

        # Step 3: Apply token-level corruptions
        if random.random() < 0.7:  # Also apply token-level corruptions most of the time
            corrupted_ids, _ = self.token_corruption_engine.corrupt(corrupted_ids, sigma)

        # Pad/truncate
        clean_ids = self._pad_or_truncate(clean_ids)
        corrupted_ids = self._pad_or_truncate(corrupted_ids)

        return (
            torch.tensor(clean_ids, dtype=torch.long),
            torch.tensor(corrupted_ids, dtype=torch.long),
            torch.tensor(sigma, dtype=torch.float32),
        )

    def _pad_or_truncate(self, ids: List[int]) -> List[int]:
        """Pad or truncate to max_len."""
        if len(ids) < self.max_len:
            return ids + [self.tokenizer.pad_id] * (self.max_len - len(ids))
        else:
            # Truncate but keep EOS
            return ids[:self.max_len - 1] + [self.tokenizer.eos_id]

    def decode(self, ids: torch.Tensor) -> str:
        """Decode token IDs to JSON string."""
        return self.tokenizer.detokenize(ids.tolist())


class JSONEvalDataset(Dataset):
    """
    Evaluation dataset with controlled corruption types.

    For systematic evaluation of repair capabilities across
    both token-level and text-level corruptions.
    """

    # Token-level corruption types
    TOKEN_CORRUPTIONS = [
        'delete_comma',
        'insert_comma',
        'delete_colon',
        'delete_brace',
        'delete_bracket',
        'swap_adjacent',
        'mask_token',
    ]

    # Text-level corruption types (from TEXT_CORRUPTIONS)
    TEXT_CORRUPTION_NAMES = [name for name, cat, fn in TEXT_CORRUPTIONS]

    def __init__(
        self,
        num_samples: int = 500,
        max_len: int = 128,
        corruption_types: Optional[List[str]] = None,
        include_text_corruptions: bool = True,
        sigma: float = 0.2,
        seed: int = 42,
    ):
        """
        Args:
            num_samples: Number of samples per corruption type
            max_len: Maximum sequence length
            corruption_types: List of corruption types to test (None = all)
            include_text_corruptions: If True, include text-level corruptions
            sigma: Fixed corruption intensity
            seed: Random seed
        """
        random.seed(seed)

        self.max_len = max_len
        self.sigma = sigma

        self.tokenizer = JSONTokenizer()
        self.token_corruption_engine = JSONCorruptionEngine(self.tokenizer)
        self.text_corruption_engine = TextCorruptionEngine()

        # Determine corruption types to test
        if corruption_types is None:
            corruption_types = list(self.TOKEN_CORRUPTIONS)
            if include_text_corruptions:
                corruption_types.extend(self.TEXT_CORRUPTION_NAMES)

        self.corruption_types = corruption_types

        # Generate samples: num_samples per corruption type
        self.samples = []
        samples_per_type = max(1, num_samples // len(corruption_types))

        for ctype in corruption_types:
            for _ in range(samples_per_type):
                json_str = generate_random_json(max_depth=2)
                clean_ids = self.tokenizer.tokenize(json_str)

                if ctype in self.TOKEN_CORRUPTIONS:
                    # Token-level corruption
                    corrupted_ids, applied = self.token_corruption_engine.corrupt(
                        clean_ids, sigma, corruption_type=ctype
                    )
                    self.samples.append({
                        'clean_json': json_str,
                        'clean_ids': clean_ids,
                        'corrupted_ids': corrupted_ids,
                        'corruption_type': ctype,
                        'category': 'token',
                        'applied': applied,
                    })
                else:
                    # Text-level corruption
                    corrupted_str, applied = self.text_corruption_engine.corrupt(
                        json_str, corruption_types=[ctype]
                    )
                    corrupted_ids = self.tokenizer.tokenize(corrupted_str)
                    self.samples.append({
                        'clean_json': json_str,
                        'clean_ids': clean_ids,
                        'corrupted_ids': corrupted_ids,
                        'corruption_type': ctype,
                        'category': 'text',
                        'applied': '+'.join(applied),
                    })

        random.seed()

    def __len__(self) -> int:
        return len(self.samples)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, str]:
        """
        Returns:
            clean: (max_len,) clean token sequence
            corrupted: (max_len,) corrupted token sequence
            sigma: scalar
            corruption_type: str
        """
        sample = self.samples[idx]

        clean_ids = self._pad_or_truncate(sample['clean_ids'])
        corrupted_ids = self._pad_or_truncate(sample['corrupted_ids'])

        return (
            torch.tensor(clean_ids, dtype=torch.long),
            torch.tensor(corrupted_ids, dtype=torch.long),
            torch.tensor(self.sigma, dtype=torch.float32),
            sample['corruption_type'],
        )

    def _pad_or_truncate(self, ids: List[int]) -> List[int]:
        if len(ids) < self.max_len:
            return ids + [self.tokenizer.pad_id] * (self.max_len - len(ids))
        else:
            return ids[:self.max_len - 1] + [self.tokenizer.eos_id]

    def decode(self, ids: torch.Tensor) -> str:
        return self.tokenizer.detokenize(ids.tolist())


def collate_fn(batch):
    """Collate function for DataLoader."""
    clean = torch.stack([b[0] for b in batch])
    corrupted = torch.stack([b[1] for b in batch])
    sigma = torch.stack([b[2] for b in batch])
    return clean, corrupted, sigma


# =============================================================================
# ITERATIVE TRAINING SUPPORT
# =============================================================================

def apply_single_corruption(text: str, tokenizer: JSONTokenizer = None) -> str:
    """
    Apply exactly one corruption to JSON text.

    Used for generating multi-corruption samples by chaining.
    Randomly chooses between text-level and token-level corruptions.
    """
    if tokenizer is None:
        tokenizer = JSONTokenizer()

    # 60% text-level, 40% token-level
    if random.random() < 0.6:
        # Text-level corruption
        text_engine = TextCorruptionEngine()
        corrupted, _ = text_engine.corrupt(text, num_corruptions=1)
        return corrupted
    else:
        # Token-level corruption
        token_engine = JSONCorruptionEngine(tokenizer)
        ids = tokenizer.tokenize(text)
        corrupted_ids, _ = token_engine.corrupt(ids, sigma=0.2)
        return tokenizer.detokenize(corrupted_ids)


def generate_multi_corruption_sample(
    clean_json: str,
    max_corruptions: int = 5,
    tokenizer: JSONTokenizer = None,
) -> Tuple[str, str, int]:
    """
    Apply 1-N corruptions to JSON, always targeting clean.

    For iterative training:
    - Input: heavily corrupted JSON
    - Target: always the clean original
    - Model learns to fix as much as it can per pass
    - At inference: iterate until convergence

    Args:
        clean_json: Valid JSON string
        max_corruptions: Maximum number of corruptions to apply
        tokenizer: Optional tokenizer instance

    Returns:
        (corrupted_json, clean_json, num_corruptions_applied)
    """
    if tokenizer is None:
        tokenizer = JSONTokenizer()

    n_corruptions = random.randint(1, max_corruptions)
    corrupted = clean_json

    for _ in range(n_corruptions):
        corrupted = apply_single_corruption(corrupted, tokenizer)

    return corrupted, clean_json, n_corruptions


class JSONIterativeDataset(Dataset):
    """
    Dataset for iterative multi-pass JSON repair training.

    Key differences from JSONRepairDataset:
    1. Applies multiple (1-N) corruptions per sample
    2. Target is ALWAYS clean (not partially corrupted)
    3. Designed to work with iterative inference loop

    Training philosophy:
    - Model sees heavily corrupted â†’ clean pairs
    - Learns to fix as much as possible per forward pass
    - At inference: call repeatedly until no changes
    """

    def __init__(
        self,
        num_samples: int,
        max_len: int = 64,
        max_corruptions: int = 5,
        seed: Optional[int] = None,
    ):
        """
        Args:
            num_samples: Number of training samples
            max_len: Maximum sequence length
            max_corruptions: Maximum corruptions per sample (1 to this value)
            seed: Random seed for reproducibility
        """
        if seed is not None:
            random.seed(seed)

        self.num_samples = num_samples
        self.max_len = max_len
        self.max_corruptions = max_corruptions

        self.tokenizer = JSONTokenizer()

        # Pre-generate clean JSON samples
        self.clean_jsons = []
        for _ in range(num_samples):
            if random.random() < 0.3:
                json_str = random.choice(JSON_TEMPLATES)
            else:
                json_str = generate_random_json(max_depth=3)
            self.clean_jsons.append(json_str)

        if seed is not None:
            random.seed()

    def __len__(self) -> int:
        return self.num_samples

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Returns:
            clean: (max_len,) clean token sequence (target)
            corrupted: (max_len,) multi-corrupted token sequence (input)
            n_corruptions: number of corruptions applied (for analysis)
        """
        clean_json = self.clean_jsons[idx]

        # Generate multi-corruption sample
        corrupted_json, _, n_corruptions = generate_multi_corruption_sample(
            clean_json,
            max_corruptions=self.max_corruptions,
            tokenizer=self.tokenizer,
        )

        # Tokenize
        clean_ids = self.tokenizer.tokenize(clean_json)
        corrupted_ids = self.tokenizer.tokenize(corrupted_json)

        # Pad/truncate
        clean_ids = self._pad_or_truncate(clean_ids)
        corrupted_ids = self._pad_or_truncate(corrupted_ids)

        return (
            torch.tensor(clean_ids, dtype=torch.long),
            torch.tensor(corrupted_ids, dtype=torch.long),
            torch.tensor(n_corruptions, dtype=torch.float32),
        )

    def _pad_or_truncate(self, ids: List[int]) -> List[int]:
        """Pad or truncate to max_len."""
        if len(ids) < self.max_len:
            return ids + [self.tokenizer.pad_id] * (self.max_len - len(ids))
        else:
            return ids[:self.max_len - 1] + [self.tokenizer.eos_id]

    def decode(self, ids: torch.Tensor) -> str:
        """Decode token IDs to JSON string."""
        return self.tokenizer.detokenize(ids.tolist())


def test_iterative_dataset():
    """Test the iterative training dataset."""
    print("\n=== JSONIterativeDataset Test ===")
    ds = JSONIterativeDataset(num_samples=20, max_len=64, max_corruptions=5, seed=42)

    import json

    corruption_counts = {}
    parse_success = 0

    for i in range(len(ds)):
        clean, corrupted, n_corrupt = ds[i]
        n = int(n_corrupt.item())
        corruption_counts[n] = corruption_counts.get(n, 0) + 1

        clean_str = ds.decode(clean)
        corrupted_str = ds.decode(corrupted)

        try:
            json.loads(corrupted_str)
            parse_success += 1
        except:
            pass

        if i < 5:
            print(f"\nSample {i} ({n} corruptions):")
            print(f"  Clean:     {clean_str[:50]}...")
            print(f"  Corrupted: {corrupted_str[:50]}...")

    print(f"\n\nCorruption distribution: {corruption_counts}")
    print(f"Corrupted samples that still parse: {parse_success}/{len(ds)} ({100*parse_success/len(ds):.1f}%)")


def test_dataset():
    """Test the JSON dataset."""
    print("=== JSONRepairDataset Test ===")
    ds = JSONRepairDataset(num_samples=10, max_len=64, seed=42)

    print(f"Tokenizer vocab size: {ds.tokenizer.vocab_size}")
    print()

    for i in range(5):
        clean, corrupted, sigma = ds[i]
        print(f"Sample {i}:")
        print(f"  Original JSON: {ds.clean_jsons[i][:60]}...")
        print(f"  Clean tokens: {clean[:20].tolist()}...")
        print(f"  Corrupted:    {corrupted[:20].tolist()}...")
        print(f"  Sigma: {sigma.item():.2f}")
        print(f"  Decoded clean:     {ds.decode(clean)[:60]}...")
        print(f"  Decoded corrupted: {ds.decode(corrupted)[:60]}...")
        print()

    print("=== JSONEvalDataset Test ===")
    eval_ds = JSONEvalDataset(num_samples=21, max_len=64)

    print(f"Total samples: {len(eval_ds)}")
    print()

    # Show one sample per corruption type
    seen = set()
    for i in range(len(eval_ds)):
        clean, corrupted, sigma, ctype = eval_ds[i]
        if ctype not in seen:
            seen.add(ctype)
            print(f"Corruption: {ctype}")
            print(f"  Clean:     {eval_ds.decode(clean)[:50]}...")
            print(f"  Corrupted: {eval_ds.decode(corrupted)[:50]}...")
            print()


def test_parse_success():
    """Test how often corrupted JSON fails to parse vs clean."""
    import json

    ds = JSONRepairDataset(num_samples=100, max_len=128, seed=42)

    clean_parse = 0
    corrupted_parse = 0

    for i in range(len(ds)):
        clean, corrupted, sigma = ds[i]
        clean_json = ds.decode(clean)
        corrupted_json = ds.decode(corrupted)

        try:
            json.loads(clean_json)
            clean_parse += 1
        except json.JSONDecodeError:
            pass

        try:
            json.loads(corrupted_json)
            corrupted_parse += 1
        except json.JSONDecodeError:
            pass

    print(f"\n=== Parse Success Test ===")
    print(f"Clean JSON parse rate: {clean_parse}/{len(ds)} ({100*clean_parse/len(ds):.1f}%)")
    print(f"Corrupted JSON parse rate: {corrupted_parse}/{len(ds)} ({100*corrupted_parse/len(ds):.1f}%)")


if __name__ == "__main__":
    test_dataset()
    test_parse_success()
    test_iterative_dataset()
